"""
Syst√®me de backup LLM unifi√© : OpenAI GPT-4o + Claude Sonnet 4
Utilis√© partout dans l'application pour avoir un fallback robuste
"""
import os
import logging
import json
from typing import Dict, Any, Optional, List
from pathlib import Path
from dotenv import load_dotenv

# Charger les variables d'environnement
ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# Configuration des cl√©s API
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
CLAUDE_API_KEY = os.environ.get('CLAUDE_API_KEY')

# Imports des biblioth√®ques
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    OpenAI = None

try:
    from emergentintegrations.llm.chat import LlmChat, UserMessage
    CLAUDE_AVAILABLE = True
except ImportError:
    CLAUDE_AVAILABLE = False
    LlmChat = None
    UserMessage = None

print(f"üîß LLM Backup System Initialized:")
print(f"   - OpenAI GPT-4o: {'‚úÖ' if OPENAI_AVAILABLE and OPENAI_API_KEY else '‚ùå'}")
print(f"   - Claude Sonnet 4: {'‚úÖ' if CLAUDE_AVAILABLE and CLAUDE_API_KEY else '‚ùå'}")


class LLMBackupSystem:
    """Syst√®me de backup LLM avec OpenAI + Claude et s√©lection intelligente selon objectifs"""
    
    def __init__(self):
        self.openai_client = None
        self.claude_chat = None
        
        # Initialiser OpenAI si disponible
        if OPENAI_AVAILABLE and OPENAI_API_KEY:
            try:
                self.openai_client = OpenAI(api_key=OPENAI_API_KEY)
                print("‚úÖ OpenAI GPT-4o client initialized")
            except Exception as e:
                print(f"‚ùå OpenAI initialization error: {e}")
        
        # Initialiser Claude si disponible
        if CLAUDE_AVAILABLE and CLAUDE_API_KEY:
            try:
                self.claude_chat = LlmChat(
                    api_key=CLAUDE_API_KEY,
                    session_id="backup-system",
                    system_message="You are Claude, a helpful AI assistant. You will receive prompts that were originally designed for GPT-4o. Please provide equivalent quality responses."
                ).with_model("anthropic", "claude-4-sonnet-20250514")
                print("‚úÖ Claude Sonnet 4 client initialized")
            except Exception as e:
                print(f"‚ùå Claude initialization error: {e}")
    
    def select_primary_llm(
        self,
        business_objective: str = "equilibre",
        brand_tone: str = "professionnel", 
        platform: str = "instagram"
    ) -> tuple:
        """
        S√©lectionne le LLM primaire et backup selon la strat√©gie business
        
        Args:
            business_objective: "conversion", "communaute", "equilibre" 
            brand_tone: ton de marque (si "storytelling" force Claude primary)
            platform: "instagram", "facebook", "linkedin"
            
        Returns:
            tuple: (primary_llm, backup_llm) o√π les valeurs sont "openai" ou "claude"
        """
        
        print(f"üß† LLM Selection - Objective: {business_objective}, Tone: {brand_tone}, Platform: {platform}")
        
        # R√®gle 1: Si storytelling ‚Üí TOUJOURS Claude primary
        if brand_tone == "storytelling":
            print("üìñ Storytelling detected ‚Üí Claude PRIMARY, OpenAI backup")
            return ("claude", "openai")
        
        # R√®gle 2: Objectif conversion ‚Üí OpenAI primary
        if business_objective == "conversion":
            print("üí∞ Conversion objective ‚Üí OpenAI PRIMARY, Claude backup")
            return ("openai", "claude")
        
        # R√®gle 3: Objectif communaut√© ‚Üí Claude primary  
        if business_objective == "communaute":
            print("üë• Community objective ‚Üí Claude PRIMARY, OpenAI backup")
            return ("claude", "openai")
        
        # R√®gle 4: √âquilibr√© ‚Üí Logique par plateforme
        if business_objective == "equilibre":
            if platform.lower() == "instagram":
                print("üì∏ Instagram + Equilibr√© ‚Üí OpenAI PRIMARY, Claude backup")
                return ("openai", "claude")
            elif platform.lower() in ["facebook", "linkedin"]:
                print(f"üîó {platform.title()} + Equilibr√© ‚Üí Claude PRIMARY, OpenAI backup")
                return ("claude", "openai")
        
        # D√©faut: OpenAI primary
        print("üîÑ Default selection ‚Üí OpenAI PRIMARY, Claude backup")
        return ("openai", "claude")
    
    async def generate_completion_with_strategy(
        self,
        messages: List[Dict[str, str]],
        business_objective: str = "equilibre",
        brand_tone: str = "professionnel",
        platform: str = "instagram",
        model: str = "gpt-4o",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        system_message: Optional[str] = None
    ) -> str:
        """
        G√©n√®re une completion avec s√©lection intelligente LLM selon la strat√©gie business
        """
        
        # S√©lectionner les LLM selon la strat√©gie
        primary_llm, backup_llm = self.select_primary_llm(business_objective, brand_tone, platform)
        
        # Construire le prompt pour Claude √† partir des messages OpenAI
        if system_message:
            full_prompt = f"System: {system_message}\n\n"
        else:
            full_prompt = ""
        
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            full_prompt += f"{role.capitalize()}: {content}\n\n"
        
        # Tentative 1: LLM primaire
        try:
            if primary_llm == "openai" and self.openai_client:
                logging.info(f"üöÄ Primary OpenAI (objective: {business_objective}, platform: {platform})...")
                
                response = self.openai_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                result = response.choices[0].message.content
                logging.info(f"‚úÖ OpenAI primary r√©ussi - {len(result)} chars")
                return result
                
            elif primary_llm == "claude" and self.claude_chat:
                logging.info(f"üß† Primary Claude (objective: {business_objective}, platform: {platform})...")
                
                user_message = UserMessage(text=full_prompt.strip())
                response = await self.claude_chat.send_message(user_message)
                
                logging.info(f"‚úÖ Claude primary r√©ussi - {len(response)} chars")
                return response
                
        except Exception as primary_error:
            logging.error(f"‚ùå Primary {primary_llm} √©chou√©: {primary_error}")
            print(f"‚ö†Ô∏è Primary {primary_llm} failed, trying {backup_llm} backup...")
        
        # Tentative 2: LLM backup
        try:
            if backup_llm == "openai" and self.openai_client:
                logging.info(f"üîÑ Backup OpenAI...")
                
                response = self.openai_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                result = response.choices[0].message.content
                logging.info(f"‚úÖ OpenAI backup r√©ussi - {len(result)} chars")
                print("‚úÖ OpenAI backup successful!")
                return result
                
            elif backup_llm == "claude" and self.claude_chat:
                logging.info(f"üîÑ Backup Claude...")
                
                user_message = UserMessage(text=full_prompt.strip())
                response = await self.claude_chat.send_message(user_message)
                
                logging.info(f"‚úÖ Claude backup r√©ussi - {len(response)} chars")
                print("‚úÖ Claude backup successful!")
                return response
                
        except Exception as backup_error:
            logging.error(f"‚ùå Backup {backup_llm} √©chou√©: {backup_error}")
            print(f"‚ùå Backup {backup_llm} failed: {str(backup_error)[:100]}...")
        
        # Si les deux √©chouent
        error_msg = f"Both {primary_llm} (primary) and {backup_llm} (backup) failed"
        logging.error(error_msg)
        raise Exception(error_msg)
    
    async def generate_completion(
        self,
        messages: List[Dict[str, str]],
        model: str = "gpt-4o",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        system_message: Optional[str] = None
    ) -> str:
        """
        G√©n√®re une completion avec backup automatique (mode compatible ancien syst√®me)
        Essaie OpenAI d'abord, puis Claude en cas d'√©chec
        """
        return await self.generate_completion_with_strategy(
            messages=messages,
            business_objective="equilibre",  # D√©faut compatible
            brand_tone="professionnel",
            platform="instagram",
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            system_message=system_message
        )
    
    async def analyze_website_content(
        self,
        content_data: Dict[str, Any],
        website_url: str
    ) -> Dict[str, Any]:
        """Analyse de contenu web avec backup"""
        
        # Prompt pour l'analyse de site web
        prompt = f"""
        Analyse ce site web en profondeur et r√©ponds UNIQUEMENT par un JSON structur√© avec les cl√©s suivantes:
        {{
            "analysis_summary": "Un r√©sum√© d√©taill√© de l'entreprise et de ses activit√©s (200-300 mots)",
            "key_topics": ["topic1", "topic2", "topic3", "topic4", "topic5"],
            "brand_tone": "le ton de communication (professionnel/d√©contract√©/premium/accessible)",
            "target_audience": "description du public cible principal",
            "main_services": ["service1", "service2", "service3"],
            "content_suggestions": ["suggestion1", "suggestion2", "suggestion3", "suggestion4"]
        }}

        Site web: {website_url}
        Titre de la page: {content_data.get('meta_title', 'Non d√©fini')}
        Description: {content_data.get('meta_description', 'Non d√©finie')}
        Titres H1: {content_data.get('h1_tags', [])}
        Titres H2: {content_data.get('h2_tags', [])}
        Contenu principal: {content_data.get('text_content', 'Non disponible')[:2000]}

        IMPORTANT: Sois tr√®s g√©n√©reux dans les d√©tails. Plus c'est pr√©cis et d√©taill√©, plus ce sera utile pour cr√©er du contenu marketing pertinent.
        """
        
        messages = [
            {
                "role": "system",
                "content": "Tu es un expert en analyse de contenu web et marketing digital. Tu analyses les sites web en profondeur pour cr√©er du contenu social media. R√©ponds UNIQUEMENT en JSON valide et structur√©."
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        try:
            raw_response = await self.generate_completion(
                messages=messages,
                temperature=0.7,
                max_tokens=2000
            )
            
            # Parser la r√©ponse JSON
            if raw_response and len(raw_response) > 50:
                # Nettoyer la r√©ponse pour extraire le JSON
                clean_response = raw_response.strip()
                if clean_response.startswith('```json'):
                    clean_response = clean_response[7:]
                if clean_response.endswith('```'):
                    clean_response = clean_response[:-3]
                
                analysis_result = json.loads(clean_response.strip())
                
                # Valider que les cl√©s requises sont pr√©sentes
                required_keys = ['analysis_summary', 'key_topics', 'brand_tone', 'target_audience', 'main_services', 'content_suggestions']
                if all(key in analysis_result for key in required_keys):
                    logging.info("‚úÖ Analyse LLM backup r√©ussie")
                    return analysis_result
                else:
                    raise ValueError("Missing required keys in response")
            
            raise ValueError("Response too short or empty")
            
        except Exception as e:
            logging.error(f"‚ùå LLM backup analysis failed: {e}")
            # Fallback g√©n√©rique si tout √©choue
            return self._create_fallback_analysis(content_data, website_url)
    
    def _create_fallback_analysis(self, content_data: Dict[str, Any], website_url: str) -> Dict[str, Any]:
        """Analyse de fallback basique si tous les LLM √©chouent"""
        title = content_data.get('meta_title', website_url)
        
        return {
            "analysis_summary": f"Entreprise analys√©e : {title}. Analyse technique indisponible, veuillez r√©essayer.",
            "key_topics": ["entreprise", "services", "activit√©"],
            "brand_tone": "professionnel",
            "target_audience": "clients potentiels",
            "main_services": ["services principaux"],
            "content_suggestions": [
                "Pr√©senter l'entreprise",
                "Partager l'expertise",
                "Montrer les r√©alisations",
                "Communiquer avec les clients"
            ]
        }


# Instance globale du syst√®me de backup
llm_backup = LLMBackupSystem()
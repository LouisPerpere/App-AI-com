"""
Advanced Post Generation System for Claire et Marcus
Generates intelligent social media posts based on user content, business profile, and website analysis.
"""

import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import random
import json
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv('/app/backend/.env')

from openai import OpenAI
from database import get_database

logger = logging.getLogger(__name__)

@dataclass
class PostContent:
    """Structure for a generated post"""
    visual_url: str
    visual_id: str
    title: str
    text: str
    hashtags: List[str]
    platform: str = "instagram"
    scheduled_date: datetime = None
    content_type: str = "product"  # product, backstage, value, sales, info, educational
    visual_type: str = "image"  # image, video
    status: str = "with_image"  # "with_image" or "needs_image"

@dataclass
class ContentSource:
    """Source content for post generation"""
    id: str
    title: str
    context: str
    visual_url: str
    file_type: str
    attributed_month: str = None
    source: str = "upload"  # upload, pixabay
    used: bool = False

class PostsGenerator:
    """Advanced posts generation system"""
    
    def __init__(self):
        self.db = get_database().db
        self.openai_client = None
        self._init_llm()
    
    def _init_llm(self):
        """Initialize LLM clients with backup system (OpenAI + Claude)"""
        try:
            # Importer le syst√®me de backup LLM
            from llm_backup_system import llm_backup
            self.llm_backup = llm_backup
            
            # Garder l'ancien client OpenAI pour compatibilit√©
            api_key = os.getenv('OPENAI_API_KEY')
            print(f"üîç DEBUG: API key loaded: {api_key[:20] if api_key else 'None'}...")
            
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)
                print("‚úÖ OpenAI client initialized for posts generation")
            else:
                self.openai_client = None
                print("‚ö†Ô∏è No OpenAI API key, using backup system only")
            
            print("‚úÖ LLM Backup System initialized for posts generation")
            
            self.system_message = """Tu es un r√©dacteur exp√©riment√© sp√©cialis√© dans les r√©seaux sociaux pour PME et artisans.

R√àGLES STRICTES DE R√âDACTION :
- BANNIR le style IA grandiose et artificiel typique de ChatGPT
- PAS de formules comme "D√©couvrez l'art de...", "Plongez dans l'univers de..."
- LIMITER les emojis au strict minimum (max 2-3 par post, seulement si vraiment pertinents)
- INTERDICTION ABSOLUE des ‚ú® et autres emojis "magiques"
- √âcrire comme un vrai humain, pas comme une IA
- Ton authentique et naturel, adapt√© au business
- Call-to-action simples et directs
- Hashtags pertinents (max 25, mix populaires/niche)

INTERDICTION ABSOLUE - PROMOTIONS NON DEMAND√âES :
- Ne JAMAIS inventer de promotions, r√©ductions, soldes ou offres commerciales
- Ne JAMAIS proposer de pourcentages de r√©duction sans instruction explicite
- Ne JAMAIS cr√©er de codes promo de ton propre chef
- SEULES les promotions explicitement mentionn√©es dans les notes peuvent √™tre utilis√©es
- Si aucune promotion n'est fournie, pr√©senter les produits/services SANS offre

Tu r√©ponds EXCLUSIVEMENT au format JSON exact demand√©."""
            
            print("‚úÖ OpenAI client initialized successfully")
            logger.info("‚úÖ OpenAI client initialized successfully")
            
        except Exception as e:
            print(f"‚ùå Failed to initialize OpenAI client: {str(e)}")
            logger.error(f"‚ùå Failed to initialize OpenAI client: {str(e)}")
            self.openai_client = None
    
    async def generate_posts_for_month(self, user_id: str, target_month: str, num_posts: int = 20, connected_platforms: List[str] = None) -> Dict[str, Any]:
        """
        Generate complete post calendar for a specific month for connected platforms
        
        Args:
            user_id: User ID
            target_month: Month in format "octobre_2025"
            num_posts: Number of posts to generate per platform
            connected_platforms: List of connected social platforms ['facebook', 'instagram', 'linkedin']
            
        Returns:
            Dict with generated posts and metadata
        """
        try:
            # Default to all platforms if none specified (backward compatibility)
            if connected_platforms is None:
                connected_platforms = ['facebook', 'instagram', 'linkedin']
            
            logger.info(f"üöÄ Starting post generation for user {user_id}, month {target_month}")
            logger.info(f"üì± Target platforms: {connected_platforms}")
            logger.info(f"üìä Posts per platform: {num_posts}")
            
            all_generated_posts = []
            all_scheduled_posts = []
            
            # STEP 1: Gather all source data
            source_data = self._gather_source_data(user_id, target_month)
            
            # STEP 1.5: V√©rifier les plateformes connect√©es
            connected_platforms = self._get_connected_platforms(user_id)
            
            # Nouvelle logique : pas de fallback, erreur si aucune connexion
            if not connected_platforms:
                logger.error("‚ùå Aucune plateforme connect√©e - g√©n√©ration impossible")
                return {
                    "success": False,
                    "error": "Aucune plateforme sociale connect√©e. Veuillez connecter au moins un r√©seau social avant de g√©n√©rer des posts.",
                    "error_type": "no_connected_platforms",
                    "posts_count": 0,
                    "posts": []
                }
            
            logger.info(f"üéØ Plateformes connect√©es trouv√©es: {connected_platforms}")
            
            # STEP 2: Collect available content
            available_content = self._collect_available_content(user_id, target_month)
            
            # STEP 3: Determine content mix strategy  
            content_strategy = self._determine_content_strategy(source_data, num_posts)
            
            # STEP 4: Generate posts for each connected platform independently
            all_generated_posts = []
            all_scheduled_posts = []
            
            for platform in connected_platforms:
                logger.info(f"üéØ G√©n√©ration pour la plateforme: {platform}")
                
                # Generate posts for this specific platform
                platform_posts = await self._generate_posts_with_strategy(
                    source_data, available_content, content_strategy, num_posts, user_id, platform
                )
                
                # Mark used content with timestamps
                await self._mark_used_content(platform_posts, platform)
                
                # Create posting schedule for this platform
                platform_scheduled_posts = self._create_posting_schedule(platform_posts, target_month, platform)
                
                # Save to database
                self._save_generated_posts(user_id, platform_scheduled_posts)
                
                all_generated_posts.extend(platform_posts)
                all_scheduled_posts.extend(platform_scheduled_posts)
                
                logger.info(f"‚úÖ Generated {len(platform_scheduled_posts)} posts for {platform}")
            
            logger.info(f"‚úÖ Generated {len(all_scheduled_posts)} posts total across {len(connected_platforms)} platforms")
            
            return {
                "success": True,
                "posts_count": len(all_scheduled_posts),
                "posts": all_scheduled_posts,
                "platforms": connected_platforms,
                "strategy": content_strategy,
                "sources_used": {
                    "business_profile": source_data.get("business_profile") is not None,
                    "website_analysis": source_data.get("website_analysis") is not None,
                    "month_content": len(source_data.get("month_content", [])),
                    "always_valid_notes": len(source_data.get("always_valid_notes", [])),
                    "month_notes": len(source_data.get("month_notes", [])),
                    "older_content": len(source_data.get("older_content", [])),
                    "pixabay_searches": source_data.get("pixabay_used", 0)
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå Post generation failed: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "posts_count": 0,
                "posts": []
            }
    
    def _get_connected_platforms(self, user_id: str) -> List[str]:
        """R√©cup√©rer toutes les plateformes connect√©es pour l'utilisateur"""
        try:
            # R√©cup√©rer les connexions sociales actives
            social_connections = list(self.db.social_connections.find({
                "user_id": user_id,
                "is_active": True
            }))
            
            connected_platforms = [conn.get("platform") for conn in social_connections if conn.get("platform")]
            logger.info(f"üîó Plateformes connect√©es: {connected_platforms}")
            
            return connected_platforms
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration plateformes: {str(e)}")
            return []

    def _gather_source_data(self, user_id: str, target_month: str) -> Dict[str, Any]:
        """Gather all source data for post generation"""
        logger.info("üìä Step 1/6: Gathering source data...")
        
        source_data = {}
        
        # Business profile
        business_profile = self.db.business_profiles.find_one({"user_id": user_id})
        source_data["business_profile"] = business_profile
        logger.info(f"   üìã Business profile: {'‚úÖ' if business_profile else '‚ùå'}")
        
        # Website analysis
        website_analysis = self.db.website_analyses.find_one(
            {"user_id": user_id}, 
            sort=[("created_at", -1)]
        )
        source_data["website_analysis"] = website_analysis
        logger.info(f"   üåê Website analysis: {'‚úÖ' if website_analysis else '‚ùå'}")
        
        # Always valid notes
        always_valid_notes = list(self.db.content_notes.find({
            "owner_id": user_id,
            "is_monthly_note": True,
            "deleted": {"$ne": True}
        }).limit(100))
        source_data["always_valid_notes"] = always_valid_notes
        logger.info(f"   üìù Always valid notes: {len(always_valid_notes)}")
        
        # Month-specific notes  
        month_notes = list(self.db.content_notes.find({
            "owner_id": user_id,
            "is_monthly_note": False,
            "note_month": self._parse_month_number(target_month),
            "note_year": self._parse_year(target_month),
            "deleted": {"$ne": True}
        }).limit(100))
        source_data["month_notes"] = month_notes
        logger.info(f"   üìÖ Month notes: {len(month_notes)}")
        
        return source_data
    
    def _parse_target_month(self, target_month: str) -> tuple:
        """Parse target_month string (e.g., 'septembre_2025') into month number and year"""
        try:
            # Split by underscore to get month and year
            parts = target_month.split('_')
            if len(parts) != 2:
                logger.warning(f"   ‚ö†Ô∏è Invalid target_month format: {target_month}, using current month")
                current_date = datetime.now()
                return current_date.month, current_date.year
            
            month_name, year_str = parts
            year = int(year_str)
            
            # French month name to number mapping
            french_months = {
                "janvier": 1, "f√©vrier": 2, "mars": 3, "avril": 4, "mai": 5, "juin": 6,
                "juillet": 7, "ao√ªt": 8, "septembre": 9, "octobre": 10, "novembre": 11, "d√©cembre": 12
            }
            
            month_num = french_months.get(month_name.lower(), datetime.now().month)
            
            logger.info(f"   üìÖ Parsed '{target_month}' -> month: {month_num}, year: {year}")
            return month_num, year
            
        except Exception as e:
            logger.error(f"   ‚ùå Error parsing target_month '{target_month}': {e}")
            current_date = datetime.now()
            return current_date.month, current_date.year

    def _collect_available_content(self, user_id: str, target_month: str) -> Dict[str, List[ContentSource]]:
        """Collect all available visual content in priority order, grouping carousels"""
        logger.info("üñºÔ∏è Step 2/6: Collecting available content...")
        logger.info(f"   üéØ Target month: {target_month}")
        
        content = {
            "month_content": [],
            "older_content": [],
            "pixabay_candidates": [],
            "carousels": []
        }
        
        # Get ALL media for this user, sorted by priority
        logger.info(f"   üîç Searching for all media with owner_id: {user_id}")
        
        all_media = list(self.db.media.find({
            "owner_id": user_id,
            "deleted": {"$ne": True}
        }).sort([("created_at", -1)]).limit(100))
        
        logger.info(f"   üìÇ Total media found: {len(all_media)}")
        
        # PRIORITY SORTING: Group by month relevance
        month_specific_media = []  # Media attributed to target_month
        current_month_media = []   # Media from current month (if different from target)
        other_media = []          # All other media
        
        # Extract month from target_month (e.g., "septembre_2025" -> 9, 2025)
        target_month_num, target_year = self._parse_target_month(target_month)
        current_date = datetime.now()
        current_month_num = current_date.month
        current_year = current_date.year
        
        logger.info(f"   üìÖ Target month parsed: {target_month_num}/{target_year}")
        logger.info(f"   üìÖ Current month: {current_month_num}/{current_year}")
        
        for item in all_media:
            attributed_month = item.get("attributed_month", "")
            
            # Check if explicitly attributed to target month
            if attributed_month == target_month:
                month_specific_media.append(item)
                logger.info(f"   ‚úÖ Found media attributed to {target_month}: {item.get('title', 'Untitled')}")
            # If not attributed but we're generating for current month, include recent uploads
            elif (target_month_num == current_month_num and target_year == current_year and 
                  not attributed_month):
                current_month_media.append(item)
                logger.info(f"   üìç Found current month media: {item.get('title', 'Untitled')}")
            else:
                other_media.append(item)
        
        logger.info("   üìä Priority distribution:")
        logger.info(f"      - Month-specific media: {len(month_specific_media)}")
        logger.info(f"      - Current month media: {len(current_month_media)}")
        logger.info(f"      - Other media: {len(other_media)}")
        
        # Process in priority order: month-specific first, then current month, then others
        prioritized_media = month_specific_media + current_month_media + other_media
        
        # Group media by carousel_id
        carousel_groups = {}
        standalone_media = []
        
        for item in prioritized_media:
            carousel_id = item.get("carousel_id")
            if carousel_id:
                if carousel_id not in carousel_groups:
                    carousel_groups[carousel_id] = []
                carousel_groups[carousel_id].append(item)
            else:
                standalone_media.append(item)
        
        logger.info(f"   üé† Found {len(carousel_groups)} carousels and {len(standalone_media)} standalone images")
        
        # Process carousels first (create one ContentSource per carousel)
        for carousel_id, carousel_items in carousel_groups.items():
            if len(carousel_items) > 1:  # Only treat as carousel if multiple images
                # Use the first image as primary for the carousel
                primary_item = carousel_items[0]
                common_title = primary_item.get("common_title", "")
                
                # Create ContentSource for the entire carousel
                title = common_title or primary_item.get("title") or f"Carrousel {carousel_id[:8]}"
                context = primary_item.get("context") or primary_item.get("description", "")
                
                carousel_source = ContentSource(
                    id=carousel_id,
                    title=title,
                    context=f"Carrousel de {len(carousel_items)} images: {context}",
                    visual_url=primary_item.get("s3_url", "") or primary_item.get("url", ""),
                    file_type="carousel",
                    attributed_month=primary_item.get("attributed_month", target_month),
                    source="carousel",
                    used=False
                )
                
                content["carousels"].append(carousel_source)
                logger.info(f"   üé† Added carousel '{title}' with {len(carousel_items)} images")
            else:
                # Single image "carousel" - treat as standalone
                standalone_media.extend(carousel_items)
        
        # Process standalone media with priority handling
        for item in standalone_media:
            # Use filename as title if no title, description as context if available
            original_filename = item.get("original_filename", "")
            # Clean filename for title (remove pixabay prefix and extensions)
            title = item.get("title") or original_filename.replace('pixabay_', '').replace('.jpg', '').replace('.jpeg', '').replace('.png', '')
            context = item.get("context") or item.get("description", "")
            
            # Truncate long context for better AI processing
            if context and len(context) > 200:
                context = context[:200] + "..."
            
            # CRITICAL FIX: Use the actual file_id for images, not MongoDB _id
            actual_file_id = None
            visual_url = item.get("url", "")
            
            # Debug logging to understand ID extraction
            logger.info(f"   üîç DEBUG: Processing item with URL: {visual_url}")
            
            if visual_url:
                # Extract file_id from URL like "/api/content/74bc5825-fec4-4696-8235-1bcb9bf20001/file"
                import re
                match = re.search(r'/api/content/([^/]+)/file', visual_url)
                if match:
                    actual_file_id = match.group(1)
                    logger.info(f"   ‚úÖ DEBUG: Extracted ID: {actual_file_id}")
                else:
                    logger.warning(f"   ‚ö†Ô∏è DEBUG: Could not extract ID from URL: {visual_url}")
                    # Fallback: use the item's id field directly
                    actual_file_id = item.get("id", str(item.get("_id", "")))
                    logger.info(f"   üîÑ DEBUG: Using fallback ID: {actual_file_id}")
            else:
                # Fallback: use the item's id field directly
                actual_file_id = item.get("id", str(item.get("_id", "")))
                logger.warning(f"   ‚ö†Ô∏è DEBUG: No URL found, using direct ID: {actual_file_id}")
                
            logger.info(f"   üìã DEBUG: Final ID to use: {actual_file_id}")
                
            content["month_content"].append(ContentSource(
                id=actual_file_id if actual_file_id else str(item.get("_id", "")),
                title=title if title else "Photo",
                context=context if context else "Image pour votre business",
                visual_url=visual_url,
                file_type=item.get("file_type", "image/jpeg"),
                attributed_month=target_month,  # Assign to current month
                source=item.get("source", "upload")  # Keep source info (pixabay vs upload)
            ))
        
        logger.info(f"   ‚úÖ FINAL month content available: {len(content['month_content'])}")
        
        # Log first few items for debugging
        for i, item in enumerate(content["month_content"][:3]):
            logger.info(f"   üì∏ Content {i+1}: ID={item.id}, Title='{item.title}', Context='{item.context[:50]}...'")
        
        return content
    
    def _determine_content_strategy(self, source_data: Dict, num_posts: int) -> Dict[str, Any]:
        """Determine optimal content mix based on business type"""
        logger.info("üéØ Step 3/6: Determining content strategy...")
        
        business_profile = source_data.get("business_profile") or {}
        business_type = business_profile.get("business_type", "service")
        
        # Content type distribution based on business type
        if business_type == "ecommerce":
            content_mix = {
                "product": 0.40,  # Product showcase
                "backstage": 0.15,  # Behind the scenes
                "value": 0.20,  # Value/tips
                "sales": 0.15,  # Direct sales
                "educational": 0.10  # Educational content
            }
        elif business_type == "service":
            content_mix = {
                "value": 0.35,  # Value/expertise
                "educational": 0.25,  # Educational
                "backstage": 0.20,  # Behind the scenes
                "sales": 0.10,  # Service promotion
                "product": 0.10  # Service showcase
            }
        elif business_type == "restaurant":
            content_mix = {
                "product": 0.35,  # Food/dishes
                "backstage": 0.30,  # Kitchen/preparation
                "value": 0.15,  # Food tips/recipes
                "sales": 0.10,  # Promotions
                "educational": 0.10  # Food education
            }
        else:  # Default mix
            content_mix = {
                "product": 0.30,
                "value": 0.25,
                "backstage": 0.20,
                "educational": 0.15,
                "sales": 0.10
            }
        
        # Calculate actual numbers
        strategy = {}
        remaining = num_posts
        
        for content_type, ratio in content_mix.items():
            count = max(1, int(num_posts * ratio))
            strategy[content_type] = min(count, remaining)
            remaining -= strategy[content_type]
        
        # Distribute remaining posts
        while remaining > 0:
            for content_type in content_mix.keys():
                if remaining > 0:
                    strategy[content_type] += 1
                    remaining -= 1
                else:
                    break
        
        logger.info(f"   üìä Content strategy: {strategy}")
        return strategy
    
    async def _generate_posts_with_strategy(self, source_data: Dict, available_content: Dict, 
                                          strategy: Dict, num_posts: int, user_id: str, target_platform: str = "instagram") -> List[PostContent]:
        """Generate posts according to the determined strategy with ONE global ChatGPT request"""
        logger.info("‚ú® Step 4/6: Generating posts with AI...")
        logger.info(f"üöÄ NEW APPROACH: Single global request for {num_posts} posts instead of individual requests")
        
        if not self.openai_client:
            raise Exception("OpenAI client not initialized")
        
        # Get contexts for AI
        business_context = self._format_business_context(source_data.get("business_profile"))
        all_notes = source_data.get("always_valid_notes", []) + source_data.get("month_notes", [])
        notes_context = self._format_notes_context(all_notes)
        recent_posts_context = await self._get_recent_posts_context(user_id)
        
        # Prepare content inventory for AI
        content_inventory = self._prepare_content_inventory(available_content)
        
        # Generate ALL posts with a single global request
        generated_posts = await self._generate_posts_calendar(
            business_context=business_context,
            notes_context=notes_context,
            recent_posts_context=recent_posts_context,
            content_inventory=content_inventory,
            strategy=strategy,
            num_posts=num_posts,
            target_platform=target_platform,
            source_data=source_data,
            available_content=available_content
        )
        
        return generated_posts
    
    async def _mark_used_content(self, generated_posts: List[PostContent], platform: str):
        """Mark used content with timestamps and platform info"""
        logger.info(f"üè∑Ô∏è Step 4.5/6: Marking used content for {platform}...")
        
        # Collect all used content IDs from generated posts
        used_content = []
        for post in generated_posts:
            if post.visual_id:
                used_content.append(post.visual_id)
        
        logger.info(f"   üìä Found {len(used_content)} content items to mark as used for {platform}")
        
        # Mark used content with timestamps and platform info
        if used_content:
            from bson import ObjectId
            from datetime import datetime
            
            # Map platform names to field names
            platform_field_map = {
                'facebook': 'used_on_facebook',
                'instagram': 'used_on_instagram', 
                'linkedin': 'used_on_linkedin'
            }
            
            platform_field = platform_field_map.get(platform.lower(), 'used_in_posts')
            
            # Handle both ObjectId and UUID formats
            for content_id in used_content:
                try:
                    # Try UUID format first (GridFS file_id)
                    update_fields = {
                        "used_in_posts": True,  # Keep general flag for backward compatibility
                        platform_field: True,  # Mark as used on specific platform
                        "last_used": datetime.utcnow().isoformat(),
                        "usage_count": {"$inc": 1}
                    }
                    
                    result = self.db.media.update_one(
                        {"$or": [
                            {"file_id": content_id},
                            {"id": content_id},
                            {"_id": ObjectId(content_id) if len(content_id) == 24 else None}
                        ]},
                        {"$set": update_fields}
                    )
                    
                    if result.matched_count > 0:
                        logger.info(f"   ‚úÖ Marked content {content_id} as used on {platform}")
                    else:
                        logger.warning(f"   ‚ö†Ô∏è Content {content_id} not found for marking as used on {platform}")
                        
                except Exception as e:
                    logger.error(f"   ‚ùå Error marking content {content_id} as used on {platform}: {str(e)}")
        
        logger.info(f"‚úÖ Marking used content completed for {platform}")
    
    def _build_business_context(self, business_profile: Dict, website_analysis: Dict) -> str:
        """Build business context for AI generation"""
        context_parts = []
        
        if business_profile:
            context_parts.append(f"ENTREPRISE: {business_profile.get('business_name', 'Non d√©fini')}")
            context_parts.append(f"TYPE: {business_profile.get('business_type', 'Non d√©fini')}")
            context_parts.append(f"DESCRIPTION: {business_profile.get('business_description', 'Non d√©finie')}")
            
            target_audience = business_profile.get('target_audience', '')
            if target_audience:
                context_parts.append(f"CIBLE: {target_audience}")
            
        if website_analysis:
            analysis_data = website_analysis.get('analysis', {})
            if analysis_data.get('target_audience_suggestions'):
                context_parts.append(f"CIBLE D√âTECT√âE: {analysis_data['target_audience_suggestions']}")
            if analysis_data.get('business_description'):
                context_parts.append(f"ACTIVIT√â D√âTECT√âE: {analysis_data['business_description']}")
        
        return "\n".join(context_parts)
    
    def _build_notes_context(self, always_valid_notes: List, month_notes: List) -> str:
        """Build notes context for AI generation"""
        context_parts = []
        
        if always_valid_notes:
            context_parts.append("NOTES TOUJOURS VALIDES:")
            for note in always_valid_notes[:5]:  # Limit to avoid token overflow
                context_parts.append(f"- {note.get('title', '')}: {note.get('content', '')}")
        
        if month_notes:
            context_parts.append("NOTES DU MOIS:")
            for note in month_notes[:5]:
                context_parts.append(f"- {note.get('title', '')}: {note.get('content', '')}")
        
        return "\n".join(context_parts)
    
    async def _get_recent_posts_context(self, user_id: str) -> str:
        """Get context from recent posts to avoid duplication"""
        try:
            # Get recent posts from last 60 days
            from datetime import datetime, timedelta
            cutoff_date = datetime.utcnow() - timedelta(days=60)
            
            recent_posts = list(self.db.generated_posts.find({
                "owner_id": user_id,
                "created_at": {"$gte": cutoff_date.isoformat()}
            }).sort([("created_at", -1)]).limit(20))
            
            if not recent_posts:
                return "Aucun post r√©cent √† √©viter."
            
            # Format recent posts for context
            context_parts = ["POSTS R√âCENTS √Ä √âVITER (ne pas copier ou r√©p√©ter):"]
            for i, post in enumerate(recent_posts[:10], 1):  # Max 10 recent posts
                title = post.get("title", "")[:50]
                text = post.get("text", "")[:100]
                context_parts.append(f"{i}. {title}: {text}...")
            
            return "\n".join(context_parts)
            
        except Exception as e:
            logger.error(f"‚ùå Error getting recent posts context: {str(e)}")
            return "Erreur lors de la r√©cup√©ration des posts r√©cents."

    def _format_business_context(self, business_profile: Dict) -> str:
        """Format complete business profile for AI context - ALL DATA"""
        if not business_profile:
            return "Aucun profil business disponible."
        
        context_parts = ["=== PROFIL BUSINESS COMPLET ==="]
        
        # Basic business info
        context_parts.append(f"Nom de l'entreprise: {business_profile.get('business_name', 'Non d√©fini')}")
        context_parts.append(f"Type d'entreprise: {business_profile.get('business_type', 'Non d√©fini')}")
        context_parts.append(f"Secteur d'activit√©: {business_profile.get('industry', 'Non d√©fini')}")
        
        # Detailed descriptions
        if business_profile.get('business_description'):
            context_parts.append(f"Description d√©taill√©e: {business_profile['business_description']}")
        
        if business_profile.get('value_proposition'):
            context_parts.append(f"Proposition de valeur: {business_profile['value_proposition']}")
        
        # Target audience details
        if business_profile.get('target_audience'):
            context_parts.append(f"Public cible: {business_profile['target_audience']}")
            
        if business_profile.get('target_audience_details'):
            context_parts.append(f"D√©tails audience: {business_profile['target_audience_details']}")
        
        # Marketing info
        if business_profile.get('brand_voice'):
            context_parts.append(f"Ton de marque: {business_profile['brand_voice']}")
            
        if business_profile.get('content_themes'):
            context_parts.append(f"Th√®mes de contenu: {business_profile['content_themes']}")
        
        # Business specifics
        if business_profile.get('products_services'):
            context_parts.append(f"Produits/Services: {business_profile['products_services']}")
            
        if business_profile.get('unique_selling_points'):
            context_parts.append(f"Points de diff√©renciation: {business_profile['unique_selling_points']}")
        
        # Goals and objectives  
        if business_profile.get('business_goals'):
            context_parts.append(f"Objectifs business: {business_profile['business_goals']}")
            
        if business_profile.get('social_media_goals'):
            context_parts.append(f"Objectifs r√©seaux sociaux: {business_profile['social_media_goals']}")
        
        # NOUVEAUX CHAMPS - Objectif de r√©sultats et strat√©gie LLM
        if business_profile.get('business_objective'):
            objective_labels = {
                'conversion': 'üí∞ Conversion (+ de ventes)',
                'communaute': 'üë• Communaut√© (+ d\'abonn√©s)', 
                'equilibre': '‚öñÔ∏è √âquilibr√© (mix ventes/abonn√©s)'
            }
            objective_desc = objective_labels.get(business_profile['business_objective'], business_profile['business_objective'])
            context_parts.append(f"üéØ OBJECTIF DE R√âSULTATS: {objective_desc}")
        
        # Posting preferences
        if business_profile.get('posting_frequency'):
            context_parts.append(f"Fr√©quence de publication: {business_profile['posting_frequency']}")
            
        if business_profile.get('preferred_post_types'):
            context_parts.append(f"Types de posts pr√©f√©r√©s: {business_profile['preferred_post_types']}")
        
        # Additional context
        if business_profile.get('competitors'):
            context_parts.append(f"Concurrents: {business_profile['competitors']}")
            
        if business_profile.get('location'):
            context_parts.append(f"Localisation: {business_profile['location']}")
        
        return "\n".join(context_parts)
    
    def _format_notes_context(self, notes: List) -> str:
        """Format notes for the global prompt"""
        if not notes:
            return "Aucune note disponible."
        
        context_parts = []
        for note in notes[:10]:  # Limit to avoid token overflow
            # Use 'description' field instead of 'title' as notes have description, not title
            description = note.get('description', '') or note.get('title', '')
            content = note.get('content', '')
            if description and content:
                context_parts.append(f"- {description}: {content}")
            elif content:
                context_parts.append(f"- {content}")
        
        return "\n".join(context_parts)
    
    def _format_website_analysis_context(self, website_analysis: Dict) -> str:
        """Format complete website analysis for AI context - ALL DATA"""
        if not website_analysis:
            return "Aucune analyse de site web disponible."
        
        context_parts = ["=== ANALYSE COMPL√àTE DU SITE WEB ==="]
        
        # Basic site info
        if website_analysis.get('url'):
            context_parts.append(f"URL du site: {website_analysis['url']}")
            
        if website_analysis.get('title'):
            context_parts.append(f"Titre du site: {website_analysis['title']}")
            
        if website_analysis.get('description'):
            context_parts.append(f"Description: {website_analysis['description']}")
        
        # Content analysis
        if website_analysis.get('main_content'):
            context_parts.append(f"Contenu principal: {website_analysis['main_content']}")
            
        if website_analysis.get('key_topics'):
            context_parts.append(f"Sujets cl√©s: {website_analysis['key_topics']}")
            
        if website_analysis.get('keywords'):
            context_parts.append(f"Mots-cl√©s: {website_analysis['keywords']}")
        
        # Business insights
        if website_analysis.get('business_model'):
            context_parts.append(f"Mod√®le √©conomique: {website_analysis['business_model']}")
            
        if website_analysis.get('target_market'):
            context_parts.append(f"March√© cible: {website_analysis['target_market']}")
            
        if website_analysis.get('value_proposition'):
            context_parts.append(f"Proposition de valeur (site): {website_analysis['value_proposition']}")
        
        # Products/Services
        if website_analysis.get('products_services'):
            context_parts.append(f"Produits/Services (site): {website_analysis['products_services']}")
            
        if website_analysis.get('unique_features'):
            context_parts.append(f"Caract√©ristiques uniques: {website_analysis['unique_features']}")
        
        # Marketing insights
        if website_analysis.get('content_strategy_suggestions'):
            context_parts.append(f"Suggestions strat√©gie contenu: {website_analysis['content_strategy_suggestions']}")
            
        if website_analysis.get('social_media_angles'):
            context_parts.append(f"Angles r√©seaux sociaux: {website_analysis['social_media_angles']}")
            
        if website_analysis.get('brand_personality'):
            context_parts.append(f"Personnalit√© de marque: {website_analysis['brand_personality']}")
        
        # Additional insights
        if website_analysis.get('competitive_advantages'):
            context_parts.append(f"Avantages concurrentiels: {website_analysis['competitive_advantages']}")
            
        if website_analysis.get('customer_pain_points'):
            context_parts.append(f"Points de douleur clients: {website_analysis['customer_pain_points']}")
            
        if website_analysis.get('content_opportunities'):
            context_parts.append(f"Opportunit√©s de contenu: {website_analysis['content_opportunities']}")
        
        # ‚ú® NOUVELLE SECTION : Analyse Storytelling Claude Sonnet 4
        if website_analysis.get('storytelling_analysis'):
            context_parts.append("\n=== üé≠ ANALYSE STORYTELLING (Claude Sonnet 4) ===")
            context_parts.append(f"Analyse narrative et storytelling: {website_analysis['storytelling_analysis']}")
            context_parts.append(f"IA utilis√©e pour storytelling: {website_analysis.get('storytelling_ai', 'Claude Sonnet 4')}")
        
        return "\n".join(context_parts)
    
    def _prepare_content_inventory(self, available_content: Dict) -> str:
        """Prepare content inventory description for AI"""
        inventory_parts = []
        
        month_content = available_content.get("month_content", [])
        older_content = available_content.get("older_content", [])
        
        inventory_parts.append("CONTENU DISPONIBLE:")
        inventory_parts.append(f"- Contenu du mois: {len(month_content)} √©l√©ments")
        inventory_parts.append(f"- Contenu plus ancien: {len(older_content)} √©l√©ments")
        
        # Add details about available content WITH IDs
        if month_content:
            inventory_parts.append("\nCONTENU DU MOIS:")
            for i, content in enumerate(month_content[:5], 1):  # Show first 5
                inventory_parts.append(f"{i}. ID:{content.id} - {content.title} - {content.context}")
        
        if older_content:
            inventory_parts.append("\nCONTENU PLUS ANCIEN:")
            for i, content in enumerate(older_content[:5], 1):  # Show first 5
                inventory_parts.append(f"{i}. ID:{content.id} - {content.title} - {content.context}")
        
        return "\n".join(inventory_parts)
    
    async def _generate_posts_calendar(self, business_context: str, notes_context: str, 
                                     recent_posts_context: str, content_inventory: str,
                                     strategy: Dict, num_posts: int, target_platform: str, source_data: Dict = None, available_content: Dict = None) -> List[PostContent]:
        """Generate entire posts calendar with intelligent LLM selection based on business objectives"""
        try:
            # R√©cup√©rer les informations du profil business pour la s√©lection LLM
            business_profile = source_data.get("business_profile", {}) if source_data else {}
            business_objective = business_profile.get("business_objective", "equilibre")
            brand_tone = business_profile.get("brand_voice", "professionnel")
            platform = target_platform  # Utilise la plateforme d√©termin√©e selon les connexions
            
            print("üéØ Post Generation Strategy:")
            print(f"   - Business Objective: {business_objective}")
            print(f"   - Brand Tone: {brand_tone}")
            print(f"   - Platform: {platform}")
            
            # Get website analysis context from source_data
            website_analysis = source_data.get("website_analysis", {}) if source_data else {}
            website_context = self._format_website_analysis_context(website_analysis)
            
            # Build the global prompt for generating all posts at once
            prompt = f"""Tu dois cr√©er EXACTEMENT {num_posts} posts {target_platform.title()} pour ce business (PAS PLUS, PAS MOINS).

{business_context}

{website_context}

NOTES IMPORTANTES √Ä INT√âGRER OBLIGATOIREMENT:
{notes_context}

‚ö†Ô∏è INSTRUCTION CRITIQUE POUR LES NOTES ‚ö†Ô∏è
Les NOTES ci-dessus sont PRIORITAIRES et OBLIGATOIRES :
- TOUTE note mentionnant une fermeture, un √©v√©nement, une date sp√©cifique DOIT √™tre int√©gr√©e dans les posts
- TOUTE information importante (salon, concours, promotion, nouveaut√©) DOIT appara√Ætre dans au moins UN post
- Les notes du mois en cours sont PRIORIT√â ABSOLUE sur tout autre contenu
- Si une note mentionne "fermeture du 30", tu DOIS cr√©er un post pour informer les clients
- Les notes g√©n√©rales (toujours valides) doivent aussi √™tre prises en compte
- REFUSE de g√©n√©rer des posts qui ignorent ces informations importantes
- En cas de doute, PRIVIL√âGIE TOUJOURS l'information des notes

{recent_posts_context}

{content_inventory}

STRAT√âGIE DE CONTENU √Ä D√âFINIR:
Analyse ce business et d√©termine la meilleure r√©partition de types de posts selon :
- Les tendances du secteur d'activit√©
- Les pr√©f√©rences de l'audience cible
- Le type d'entreprise et ses objectifs
- L'analyse du site web et des contenus disponibles

Types de posts possibles :
- MISE EN AVANT de produits/services (product)
- CONTENU INFORMATIF et √©ducatif (educational) 
- BACKSTAGE et coulisses (backstage)
- CONTENU DE VALEUR et conseils (value)
- PROMOTION et vente (sales)
- T√âMOIGNAGES et social proof (testimonials)
- TENDANCES et actualit√©s du secteur (trends)

Choisis et r√©partis intelligemment ces types selon ce qui fonctionnera le mieux pour cette entreprise et son audience.

R√àGLES STRICTES DE R√âDACTION:
- BANNIR absolument le style IA artificiel et grandiose
- PAS de "D√©couvrez l'art de...", "Plongez dans...", "Laissez-vous s√©duire..."
- INTERDICTION des ‚ú® et emojis "magiques" 
- Maximum 2-3 emojis par post, seulement si vraiment utiles
- √âcrire comme un VRAI humain, pas comme ChatGPT
- Ton naturel et authentique
- Call-to-action simples et directs
- 150-200 mots maximum par post
- 15-25 hashtags pertinents par post

üö´ INTERDICTION FORMELLE - PROMOTIONS NON DEMAND√âES üö´
- NE JAMAIS inventer de promotions, r√©ductions, soldes ou offres commerciales
- NE JAMAIS proposer de pourcentages de r√©duction (ex: -20%, -50%)
- NE JAMAIS cr√©er de codes promo ou d'offres temporaires
- SEULES les promotions explicitement mentionn√©es dans les NOTES peuvent √™tre utilis√©es
- Si aucune promotion n'est mentionn√©e dans les notes, NE PAS en proposer
- Concentre-toi sur la pr√©sentation des produits/services sans inventer d'offres

CONTRAINTES DE VARI√âT√â CRITIQUE:
- Chaque post doit √™tre UNIQUE et suffisamment diff√©rent des autres
- Varier les angles d'approche, le vocabulaire, et la structure
- NE JAMAIS r√©p√©ter exactement le m√™me contenu ou les m√™mes formulations
- Adapter le niveau de d√©tail et le style selon le type de post
- Comme le ferait un expert community manager professionnel

üö® RAPPEL CRITIQUE - NOTES OBLIGATOIRES üö®
AVANT de g√©n√©rer tes posts, relis les NOTES ci-dessus et assure-toi qu'elles sont TOUTES prises en compte dans ta planification. Toute information importante (fermeture, √©v√©nement, nouveaut√©) DOIT appara√Ætre dans au moins un post. C'est NON-N√âGOCIABLE.

R√âPONSE ATTENDUE (JSON exact avec array de EXACTEMENT {num_posts} posts):
{{
    "posts": [
        {{
            "content_type": "product|educational|backstage|value|sales|testimonials|trends",
            "text": "Texte naturel sans style IA, max 2-3 emojis",
            "hashtags": ["hashtag1", "hashtag2", "hashtag3"],
            "title": "Titre simple et direct",
            "visual_id": "ID_exact_du_contenu_utilis√©_de_la_liste_ci-dessus",
            "scheduling_preference": "morning|afternoon|evening"
        }},
        // ... r√©p√©ter pour EXACTEMENT {num_posts} posts au total
    ]
}}

CRITIQUE: Tu DOIS retourner EXACTEMENT {num_posts} posts. Ni plus, ni moins. Compte bien!

IMPORTANT: Varie intelligemment les content_type selon ce qui sera le plus efficace pour ce business et son audience cible.
"""
            
            logger.info(f"ü§ñ Sending STRATEGIC request for {num_posts} posts")
            logger.info(f"ü§ñ Strategy: {business_objective} objective, {brand_tone} tone, {platform} platform")
            logger.info(f"ü§ñ Prompt length: {len(prompt)} characters")
            
            # Utiliser le syst√®me de backup LLM avec s√©lection intelligente
            try:
                messages = [
                    {"role": "system", "content": self.system_message},
                    {"role": "user", "content": prompt}
                ]
                
                # NOUVELLE LOGIQUE: S√©lection LLM intelligente selon objectifs business
                response_text = await self.llm_backup.generate_completion_with_strategy(
                    messages=messages,
                    business_objective=business_objective,
                    brand_tone=brand_tone,
                    platform=platform,
                    model="gpt-4o",
                    temperature=0.7,
                    max_tokens=4000
                )
                
                logger.info(f"ü§ñ Strategic LLM Response length: {len(response_text) if response_text else 0}")
                
                # CRITICAL DEBUG: Log the full response to see what it actually returns
                logger.info("üîç FULL STRATEGIC LLM RESPONSE:")
                logger.info(f"{response_text}")
                
                # Parse the global response with content mapping
                return self._parse_global_response(response_text, strategy, available_content, num_posts, target_platform)
                
            except Exception as llm_error:
                logger.error(f"‚ùå LLM backup failed: {llm_error}")
                # Fallback to direct OpenAI if backup system fails
                if self.openai_client:
                    logger.info("üîÑ Falling back to direct OpenAI...")
                    response = self.openai_client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {"role": "system", "content": self.system_message},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.7,
                        max_tokens=4000
                    )
                    response_text = response.choices[0].message.content
                    return self._parse_global_response(response_text, strategy, available_content, num_posts, target_platform)
                else:
                    raise llm_error
            
        except Exception as e:
            logger.error(f"‚ùå Failed to generate posts calendar: {str(e)}")
            return []
    
    def _format_strategy_for_prompt(self, strategy: Dict) -> str:
        """Format strategy for the AI prompt"""
        strategy_parts = []
        for content_type, count in strategy.items():
            if count > 0:
                strategy_parts.append(f"- {count} posts de type '{content_type}'")
        return "\n".join(strategy_parts)
    
    def _parse_global_response(self, response_text: str, strategy: Dict, available_content: Dict = None, num_posts: int = None, target_platform: str = "instagram") -> List[PostContent]:
        """Parse the global AI response into PostContent objects"""
        try:
            if not response_text or not response_text.strip():
                logger.error("‚ùå Empty global response from AI")
                return []
            
            # Clean response text - remove markdown code blocks if present
            clean_response = response_text.strip()
            if clean_response.startswith('```json'):
                clean_response = clean_response[7:]  # Remove ```json
            if clean_response.endswith('```'):
                clean_response = clean_response[:-3]  # Remove ```
            clean_response = clean_response.strip()
            
            response_data = json.loads(clean_response)
            posts_data = response_data.get("posts", [])
            
            # CRITICAL VALIDATION: Ensure exact number of posts
            if num_posts and len(posts_data) != num_posts:
                logger.warning(f"‚ö†Ô∏è ChatGPT returned {len(posts_data)} posts but {num_posts} were requested!")
                if len(posts_data) > num_posts:
                    logger.info(f"üîß Truncating to first {num_posts} posts")
                    posts_data = posts_data[:num_posts]
                elif len(posts_data) < num_posts:
                    logger.error(f"‚ùå Not enough posts generated. Needed {num_posts}, got {len(posts_data)}")
                    # Could implement retry logic here if needed
            
            if not posts_data:
                logger.error("‚ùå No posts found in global AI response")
                return []
            
            # Debug: Log the first post structure
            if posts_data:
                logger.info(f"üîç DEBUG: First post structure: {posts_data[0]}")
                logger.info(f"üîç DEBUG: Visual ID in first post: {posts_data[0].get('visual_id', 'NOT_FOUND')}")
            
            # Collect all available content IDs for validation
            all_content_ids = []
            if available_content:
                logger.info(f"üîç DEBUG: available_content keys: {list(available_content.keys())}")
                for key, content_list in available_content.items():
                    logger.info(f"üîç DEBUG: {key} has {len(content_list)} items")
                    all_content_ids.extend([content.id for content in content_list])
            
            logger.info(f"üîç DEBUG: Total content IDs available: {len(all_content_ids)}")
            logger.info(f"üîç DEBUG: Content IDs: {all_content_ids[:5]}...")  # Show first 5
            
            generated_posts = []
            
            for i, post_data in enumerate(posts_data):
                # Extract the real visual_id from ChatGPT response
                visual_id = post_data.get("visual_id", "")
                
                # NEW APPROACH: Keep ALL posts, even those without visual_id
                if visual_id and visual_id in all_content_ids:
                    # Valid visual_id found - construct full URL like other images in the app
                    visual_url = f"/api/content/{visual_id}/file"
                    logger.info(f"   ‚úÖ Post {i+1}: Using REAL photo ID {visual_id}")
                    logger.info(f"   üì∏ Constructed visual_url: {visual_url}")
                    
                    post = PostContent(
                        visual_url=visual_url,
                        visual_id=visual_id,
                        title=post_data.get("title", f"Post {i+1}"),
                        text=post_data.get("text", ""),
                        hashtags=post_data.get("hashtags", []),
                        platform=target_platform,
                        content_type=post_data.get("content_type", "product"),
                        status="with_image"
                    )
                    
                    generated_posts.append(post)
                    
                else:
                    # KEEP posts without visual_id - user can add image later
                    logger.info(f"   üìù Post {i+1}: Text-only post (no visual assigned) - KEEPING for manual image assignment")
                    
                    post = PostContent(
                        visual_url="",  # Empty - will be filled by user
                        visual_id="",   # Empty - will be filled by user
                        title=post_data.get("title", f"Post {i+1}"),
                        text=post_data.get("text", ""),
                        hashtags=post_data.get("hashtags", []),
                        platform=target_platform,
                        content_type=post_data.get("content_type", "product"),
                        status="needs_image"  # Mark as needing image
                    )
                    
                    generated_posts.append(post)
                    
            # CRITICAL: Only return posts that use real photos
            logger.info(f"   ‚úÖ Final result: {len(generated_posts)} posts with REAL photos only (no fallbacks)")
            return generated_posts
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Failed to parse global AI response: {e}")
            logger.error(f"‚ùå Response was: {response_text[:500]}...")
            return []
        except Exception as e:
            logger.error(f"‚ùå Error parsing global response: {str(e)}")
            return []
    async def _generate_single_post(self, visual_content: ContentSource, content_type: str, 
                                   business_context: str, notes_context: str, user_id: str, target_platform: str = "instagram") -> Optional[PostContent]:
        """Generate a single post with AI"""
        try:
            # Get recent posts context to avoid duplication
            recent_posts_context = await self._get_recent_posts_context(user_id)
            
            # Build prompt for AI
            prompt = f"""Cr√©e un post Instagram {content_type} naturel et authentique.

CONTEXTE BUSINESS:
{business_context}

NOTES IMPORTANTES:
{notes_context}

CONTENU VISUEL:
- Titre: {visual_content.title}
- Contexte: {visual_content.context}
- Type: {visual_content.file_type}

{recent_posts_context}

TYPE DE POST: {content_type}
{self._get_content_type_guidelines(content_type)}

R√àGLES STRICTES DE R√âDACTION:
- BANNIR absolument le style IA artificiel et grandiose
- PAS de "D√©couvrez l'art de...", "Plongez dans...", "Laissez-vous s√©duire..."
- INTERDICTION des ‚ú® et emojis "magiques" 
- Maximum 2-3 emojis par post, seulement si vraiment utiles
- √âcrire comme un VRAI humain, pas comme ChatGPT
- Ton naturel et authentique
- Call-to-action simples et directs
- 150-200 mots maximum
- 15-25 hashtags pertinents

üö´ INTERDICTION FORMELLE - PROMOTIONS NON DEMAND√âES üö´
- NE JAMAIS inventer de promotions, r√©ductions, soldes ou offres commerciales
- NE JAMAIS proposer de pourcentages de r√©duction (ex: -20%, -50%)
- NE JAMAIS cr√©er de codes promo ou d'offres temporaires
- SEULES les promotions explicitement mentionn√©es dans les NOTES peuvent √™tre utilis√©es
- Si aucune promotion n'est mentionn√©e dans les notes, NE PAS en proposer
- Concentre-toi sur la pr√©sentation des produits/services sans inventer d'offres

CONTRAINTES DE VARI√âT√â CRITIQUE:
- Chaque post doit √™tre UNIQUE et suffisamment diff√©rent des autres
- Varier les angles d'approche, le vocabulaire, et la structure
- NE JAMAIS r√©p√©ter exactement le m√™me contenu ou les m√™mes formulations
- Adapter le niveau de d√©tail et le style selon le type de post
- Comme le ferait un expert community manager professionnel

üö® RAPPEL CRITIQUE - NOTES OBLIGATOIRES üö®
AVANT de g√©n√©rer tes posts, relis les NOTES ci-dessus et assure-toi qu'elles sont TOUTES prises en compte dans ta planification. Toute information importante (fermeture, √©v√©nement, nouveaut√©) DOIT appara√Ætre dans au moins un post. C'est NON-N√âGOCIABLE.

R√âPONSE ATTENDUE (JSON exact):
{{
    "text": "Texte naturel sans style IA, max 2-3 emojis",
    "hashtags": ["hashtag1", "hashtag2", "hashtag3"],
    "title": "Titre simple et direct"
}}
"""
            
            # Send to LLM Backup System (OpenAI + Claude)
            print("ü§ñ DEBUG: Sending request to LLM Backup System")
            print(f"ü§ñ DEBUG: Prompt length: {len(prompt)}")
            
            try:
                messages = [
                    {"role": "system", "content": self.system_message},
                    {"role": "user", "content": prompt}
                ]
                
                response_text = await self.llm_backup.generate_completion(
                    messages=messages,
                    model="gpt-4o",
                    temperature=0.7,
                    max_tokens=1000
                )
                
                print(f"ü§ñ DEBUG: LLM Backup response: '{response_text}'")
                print(f"ü§ñ DEBUG: Response length: {len(response_text) if response_text else 0}")
                
                logger.info(f"   ü§ñ LLM Backup Response length: {len(response_text) if response_text else 0}")
                logger.info(f"   ü§ñ LLM Response preview: {response_text[:100] if response_text else 'Empty response'}")
                
            except Exception as llm_error:
                logger.error(f"‚ùå LLM backup failed for single post: {llm_error}")
                # Fallback to direct OpenAI if backup system fails
                if self.openai_client:
                    logger.info("üîÑ Falling back to direct OpenAI for single post...")
                    response = self.openai_client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {"role": "system", "content": self.system_message},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.7,
                        max_tokens=1000
                    )
                    response_text = response.choices[0].message.content
                else:
                    raise llm_error
            
            # Parse response
            try:
                if not response_text or not response_text.strip():
                    logger.error("   ‚ùå Empty response from AI")
                    return None
                
                # Clean response text - remove markdown code blocks if present
                clean_response = response_text.strip()
                if clean_response.startswith('```json'):
                    clean_response = clean_response[7:]  # Remove ```json
                if clean_response.endswith('```'):
                    clean_response = clean_response[:-3]  # Remove ```
                clean_response = clean_response.strip()
                
                print(f"ü§ñ DEBUG: Cleaned response: '{clean_response[:200]}...'")
                    
                response_data = json.loads(clean_response)
                
                # Construct proper visual URL for frontend display
                proper_visual_url = f"/api/content/{visual_content.id}/file" if visual_content.id else ""
                
                return PostContent(
                    visual_url=proper_visual_url,
                    visual_id=visual_content.id,
                    title=response_data.get("title", visual_content.title),
                    text=response_data.get("text", ""),
                    hashtags=response_data.get("hashtags", []),
                    platform=target_platform,
                    content_type=content_type,
                    visual_type="video" if visual_content.file_type.startswith("video") else "image"
                )
                
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå Failed to parse AI response: {e}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Failed to generate post: {str(e)}")
            return None
    
    def _get_content_type_guidelines(self, content_type: str) -> str:
        """Get specific guidelines for each content type"""
        guidelines = {
            "product": """
OBJECTIF: Mettre en valeur un produit/service
STYLE: Descriptif, b√©n√©fices, caract√©ristiques
CTA: "D√©couvrez", "Commandez", "Plus d'infos"
""",
            "backstage": """
OBJECTIF: Montrer les coulisses, l'authenticit√©  
STYLE: Personnel, humain, transparent
CTA: "Suivez-nous", "Posez vos questions"
""",
            "value": """
OBJECTIF: Apporter de la valeur, conseils, astuces
STYLE: Informatif, utile, expertise
CTA: "Sauvegardez", "Partagez", "Commentez"
""",
            "sales": """
OBJECTIF: Conversion directe, promotion
STYLE: Urgence, b√©n√©fices clairs, offre
CTA: "Achetez maintenant", "Profitez", "Contactez-nous"
""",
            "educational": """
OBJECTIF: √âduquer, expliquer, former
STYLE: P√©dagogique, √©tapes, exemples
CTA: "Apprenez plus", "Essayez", "Suivez le guide"
"""
        }
        
        return guidelines.get(content_type, "")
    
    def _get_pixabay_content(self, business_context: str, content_type: str) -> Optional[ContentSource]:
        """Get Pixabay content as fallback (simplified for now)"""
        # TODO: Implement intelligent Pixabay search based on business context
        logger.info(f"   üì∏ Would search Pixabay for {content_type} content")
        return None
    
    def _create_posting_schedule(self, posts: List[PostContent], target_month: str, target_platform: str = "instagram") -> List[PostContent]:
        """Create intelligent posting schedule starting from tomorrow"""
        logger.info("üìÖ Step 5/6: Creating posting schedule...")
        
        from datetime import datetime, timedelta
        import random
        import calendar
        
        # Parse target month
        month_name, year = target_month.split('_')
        year = int(year)
        
        month_map = {
            'janvier': 1, 'f√©vrier': 2, 'mars': 3, 'avril': 4,
            'mai': 5, 'juin': 6, 'juillet': 7, 'ao√ªt': 8,
            'septembre': 9, 'octobre': 10, 'novembre': 11, 'd√©cembre': 12
        }
        
        month_num = month_map.get(month_name, 10)
        
        # Calculate start date: tomorrow or first day of target month (whichever is later)
        current_date = datetime.now()
        tomorrow = current_date + timedelta(days=1)
        first_day_of_month = datetime(year, month_num, 1)
        
        start_date = max(tomorrow, first_day_of_month)
        
        # Calculate end date: last day of target month
        last_day_of_month = calendar.monthrange(year, month_num)[1]
        end_date = datetime(year, month_num, last_day_of_month)
        
        # Calculate available days for scheduling
        available_days = (end_date - start_date).days + 1
        
        logger.info(f"   üìÖ Scheduling from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}")
        logger.info(f"   üìä Available days: {available_days}")
        
        # Optimal posting times for social media (research-based)
        optimal_hours = [9, 11, 13, 17, 19, 21]  # Best engagement hours
        
        for i, post in enumerate(posts):
            # Distribute posts evenly across available days
            if available_days > 0:
                day_offset = (i * available_days) // len(posts)
            else:
                day_offset = 0
                
            hour = random.choice(optimal_hours)
            minute = random.randint(0, 59)  # Add random minutes for natural scheduling
            
            scheduled_date = start_date + timedelta(days=day_offset, hours=hour, minutes=minute)
            
            # Ensure we don't schedule beyond the month
            if scheduled_date > end_date:
                scheduled_date = end_date.replace(hour=hour, minute=minute)
                
            post.scheduled_date = scheduled_date
            
            # Assurer que la plateforme est correcte
            post.platform = target_platform
        
        # Sort by scheduled date
        posts.sort(key=lambda p: p.scheduled_date)
        
        logger.info(f"   üìÖ Scheduled {len(posts)} posts for {target_platform} from {start_date.strftime('%d/%m')} to {end_date.strftime('%d/%m')}")
        return posts
    
    def _save_generated_posts(self, user_id: str, posts: List[PostContent]):
        """Save generated posts to database"""
        logger.info("üíæ Step 6/6: Saving generated posts...")
        
        # Clear existing posts for this month
        # TODO: Add month filtering when saving
        
        for post in posts:
            post_doc = {
                "id": f"post_{user_id}_{int(datetime.now().timestamp())}_{posts.index(post)}",
                "owner_id": user_id,
                "visual_url": post.visual_url,
                "visual_id": post.visual_id,
                "title": post.title,
                "text": post.text,
                "hashtags": post.hashtags,
                "platform": post.platform,
                "scheduled_date": post.scheduled_date.isoformat() if post.scheduled_date else None,
                "content_type": post.content_type,
                "visual_type": post.visual_type,
                "status": post.status,  # Preserve the actual status from PostContent
                "created_at": datetime.utcnow().isoformat(),
                "published": False
            }
            
            self.db.generated_posts.insert_one(post_doc)
        
        logger.info(f"   üíæ Saved {len(posts)} posts to database")
    
    def _parse_month_number(self, target_month: str) -> int:
        """Parse month number from target_month string"""
        month_name = target_month.split('_')[0]
        month_map = {
            'janvier': 1, 'f√©vrier': 2, 'mars': 3, 'avril': 4,
            'mai': 5, 'juin': 6, 'juillet': 7, 'ao√ªt': 8,
            'septembre': 9, 'octobre': 10, 'novembre': 11, 'd√©cembre': 12
        }
        return month_map.get(month_name, 10)
    
    def _parse_year(self, target_month: str) -> int:
        """Parse year from target_month string"""
        return int(target_month.split('_')[1])
    
    def _parse_target_month(self, target_month: str) -> tuple:
        """Parse target month string into month number and year"""
        month_name, year_str = target_month.split('_')
        year = int(year_str)
        
        month_map = {
            'janvier': 1, 'f√©vrier': 2, 'mars': 3, 'avril': 4,
            'mai': 5, 'juin': 6, 'juillet': 7, 'ao√ªt': 8,
            'septembre': 9, 'octobre': 10, 'novembre': 11, 'd√©cembre': 12
        }
        
        month_num = month_map.get(month_name, 10)
        return month_num, year